# The Recursive Turtle

There's an apophrychal story about a young westerner who traveled to India to 
study meditation under one of the leaders in the space. It could have been John
Lennon or Steve Jobs going to study under Maharishi or Baba. The specific time
and individuals aren't importatnt, only that some young ambitous individual
intentionally gets into a big and challenging context switch and is permanantly
altered by the experience (for better or worse). It goes soemthing like this:

> In our favored version, an Eastern guru affirms that the earth is supported 
> on the back of a tiger. When asked what supports the tiger, he says it stands
> upon an elephant; and when asked what supports the elephant he says it is a 
> giant turtle. When asked, finally, what supports the giant turtle, he is 
> briefly taken aback, but quickly replies "Ah, after that it is turtles all 
> the way down."
[src](https://en.wikipedia.org/wiki/Turtles_all_the_way_down)

I often hear (and use) this metaphor when explaning how software works. Python
"reads" a program file you write in plain text (not hugely from this blog).
Python itself is a C program that's written by a committee of people and a
compiler that translates that C program into whatever dialect of byte 
instructions your CPU prefers. Below that level, my understanding gets a bit 
hazier. In spite of my limited knowledge, I know there cannot be infinite 
turtles because the address space of every extant computer (that I'm aware of)
is finite.

## How do infinite turtles constrain themselves into a finite space?

I'm not sure, but this question is especially cruel in Computer Science, 
because of it's relative tractability. In other domains like Physics or Biology,
there's an expanding horizon between what is known, and what is yet to be known.
I'm not sure this horizon exists in Computer Science, it is "a science in a 
bottle." 

90% of the discoveries in the field came after 1947. Virtually all of those
discoveries are based on the work of a few great minds including but definitely
not limited to: Claude Shannon, Grace Hopper, and John Von Neumann. Every great
idea in Computer Science is composed of pieces that came before it. That is to
say, every great idea in Computer Science is not only theoretically knowable (
like in Biology), but _was practically known_ as a matter of course, by someone
not too far in time and space from you. None of these great minds lived in
semi-mythical eras with spotty records, like Confucious or Moses or Augustus,
thier works are historically 
_(right there)[https://ieeexplore.ieee.org/document/6773024], less than a 
century's drive away_. In theory, one could go and read thier seminal works to
completion. 

This is why it's so challenging to understand (or let alone mentor a junior
colleauge). The bounds of knowledge are practically finite, and any
intractability is due to an inability to follow and understand the underying
concepts (due to limited time or other contstraints placed on office workers).

I don't have a grand conclusion here, so if you've read this far I owe you some 
grattitude and an apology. I've just been wrestling with these ideas while building
my first 'real' project in a compiled languge 
([take a peek](https://www.github.com/BenDavidAaron/gocart)) mixed in with a
shot of impostor syndrome and a dash of seasonal affective disorder. Thanks
for reading, hopefully the quality of writing here will improve with practice. 
